{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr Dennett's guide to Spatial Interaction Modelling - Part 1: The Unconstrained (Total Constrained) Model\n",
    "Code  translated from R to Python by Philip Wilkinson and revised by Elsa Arcaute and Mateo Neira.\n",
    "\n",
    "We are going to assume that you are familiar with Python and anaconda. If not please see the first set of practicals. \n",
    "\n",
    "We should already have the Urbsim environment installed and we will proceed on the basis that we are using this kernel. As such, all packages that are required should already be installed and we can simply import all the packages that we may need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up some spatial data\n",
    "\n",
    "AS the name suggests, to run a spatial model you are going to need some spatial data and some data on interactions (flows). Let's start with some spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch a GeoJson of some district-level boundaries from the ONS Geoportal. First add the URL to an object\n",
    "url = \"http://geoportal.statistics.gov.uk/datasets/8edafbe3276d4b56aec60991cbddda50_2.geojson\"\n",
    "EW = gpd.read_file(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure the boundaries have downloaded OK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EW.plot(figsize = (10,15), facecolor = \"None\", edgecolor = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EW.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being let us extract London which corresponds to \"E09\" from the \"lad15cd\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London = EW[EW.lad15cd.str.contains(\"E09\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London.plot(figsize = (10,15), facecolor = \"None\", edgecolor = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look under the bonnet\n",
    "London.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculting a distance matrix\n",
    "\n",
    "Now we have a nice clean set of London Boundaries, let's extract some spatial data. Of course, the important spatial data for spatial interaction models relates to the cost of interaction between places and this is very frequently represented through distance...\n",
    "\n",
    "If you look at the crs above, the boundaries are not in the British National Grid Projection, as we can see that the CRS is WGS84, indicating longitude and latitude coordinates. We need to change this to the British National grid so that our distances are in metres and not decimal degrees, then we need to generate a distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform london to BNG\n",
    "London.to_crs(\"EPSG:27700\", inplace = True)\n",
    "#We can check that this has occured by plotting the results\n",
    "London.plot(figsize = (10,15), facecolor = \"None\", edgecolor = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order it by borough code - *This step will be important later on*\n",
    "London.sort_values(by=[\"lad15cd\"], inplace = True)\n",
    "\n",
    "#extract the centroids\n",
    "London[\"Centroids\"] = London.geometry.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "London.set_index('lad15cd', inplace=True, drop=True)\n",
    "#calculate distances from all borough centroids to all borough centroids\n",
    "# note the use of lambda to define a function\n",
    "distances = London.rename_axis('Orig').Centroids.apply(lambda x: London.rename_axis('Dest').Centroids.distance(x)).stack().reset_index()\n",
    "distances.rename(columns = {0:'Dist'}, inplace=True)\n",
    "distances.sort_values(by=[\"Orig\", \"Dest\"], inplace = True)\n",
    "London.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given that we have converted to the BNG already, this means that we could simply compute the Euclidean distance\n",
    "#which can be done with this code\n",
    "#uncomment if you wish\n",
    "\n",
    "#drop any uncessary columns\n",
    "#LDN_cent = London.drop(columns = [\"objectid\", \"lad15nm\", \"lad15nmw\", \"st_areashape\", \"st_lengthshape\", \"geometry\"])\n",
    "\n",
    "#extract the x and y coordinates\n",
    "#LDN_cent[\"x\"] = LDN_cent[\"Centroids\"].map(lambda p: p.x)\n",
    "#LDN_cent[\"y\"] = LDN_cent[\"Centroids\"].map(lambda p: p.y) \n",
    "\n",
    "#you can now drop the centroids\n",
    "#LDN_cent.drop(columns =[ \"Centroids\"], inplace = True)\n",
    "\n",
    "#extract the distances for each possible pair\n",
    "#distances = []\n",
    "#for index1, row1 in LDN_cent.iterrows():\n",
    "#    for index2, row2 in LDN_cent.iterrows():\n",
    "#        #append the orig code, destination code and distance\n",
    "#        distances.append([row1[\"lad15cd\"], row2[\"lad15cd\"], \n",
    "#                          ((float(row1[\"x\"])-float(row2[\"x\"]))**2 +(float(row1[\"y\"])-float(row2[\"y\"]))**2)**0.5])\n",
    "        \n",
    "#convert the resulting list into a dataframe        \n",
    "#distances = pd.DataFrame.from_records(distances, columns = [\"Orig\", \"Dest\", \"Dist\"])\n",
    "\n",
    "#sort by origin code and then destination code\n",
    "#distances.sort_values(by=[\"Orig\", \"Dest\"], inplace = True)\n",
    "\n",
    "#check the results\n",
    "#distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow data\n",
    "\n",
    "The data we are going to use to test our spatial interaction models is with commuting data from the 2001 England and Wales Census. In the Census, there is a question which asks your home address and another which asks the location of your usual place of work, as well as the method of transport that you mainly use to conduct your commute. From this, estimates of commuter flows by transportation type for the whole country can be generated.\n",
    "\n",
    "In this exercise, to save time, I have already downloaded some sample data which records the place (Borough) or residence and place (Borough) of work for all people living in London at the time of the 2001 Census. Borough level is quite coarse but it will suffice for demonstration purposes. If you would like to download your own commuting or migration flow data, then you should visit the Census Support Flow Data Service called wicid - https://wicid.ukdataservice.ac.uk/ - here you can download flows for a huge range of geographies from the 1981, 1991, 2001 and 2011 Censuses.\n",
    "\n",
    "As well as flow data, I have also collected some additional data on income, the number of jobs in each borough in 2001 and the total population - we will uses these as destination atractiveness/mass term and origin emissiveness/mass term proxies in the models which follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in your London Commuting Data\n",
    "cdata = pd.read_csv(\"https://www.dropbox.com/s/7c1fi1txbvhdqby/LondonCommuting2001.csv?raw=1\")\n",
    "#read in a lookup table for translating between old borough codes and new borough codes\n",
    "CodeLookup = pd.read_csv(\"https://www.dropbox.com/s/h8mpvnepdkwa1ac/CodeLookup.csv?raw=1\")\n",
    "#read in some population and income data\n",
    "popincome = pd.read_csv(\"https://www.dropbox.com/s/84z22a4wo3x2p86/popincome.csv?raw=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the new Borough code to the origin old Borough code and remove the uncessary columns\n",
    "cdata = cdata.merge(CodeLookup, left_on = \"OrigCode\", right_on = \"OldCode\")\n",
    "cdata.drop(columns = [\"OldCode\", \"Label\"], inplace = True)\n",
    "cdata.rename(columns = {\"NewCode\": \"OrigCodeNew\"}, inplace = True)\n",
    "\n",
    "#do the same with the destinations\n",
    "cdata = cdata.merge(CodeLookup, left_on = \"DestCode\", right_on=\"OldCode\")\n",
    "cdata.drop(columns = [\"OldCode\", \"Label\"], inplace = True)\n",
    "cdata.rename(columns = {\"NewCode\": \"DestCodeNew\"}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add the population income (here we will use the median income as it is a more reliable indicator and not \n",
    "#affected by skewness as much as average income might be)\n",
    "popincome.drop(columns = [\"label\", \"avg_income\"], inplace = True)\n",
    "\n",
    "#merge on the origins\n",
    "cdata = cdata.merge(popincome, left_on = \"OrigCodeNew\", right_on =\"code\")\n",
    "cdata.drop(columns = [\"code\"], inplace = True)\n",
    "cdata.rename(columns={\"pop\":\"Oi1_origpop\", \"med_income\": \"Oi2_origsal\"}, inplace = True)\n",
    "\n",
    "#do the same with the destination\n",
    "cdata = cdata.merge(popincome, left_on = \"DestCodeNew\", right_on =\"code\")\n",
    "cdata.drop(columns = [\"code\"], inplace = True)\n",
    "cdata.rename(columns={\"pop\":\"Dj1_destpop\", \"med_income\": \"Dj2_destsal\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data needs to be ordered by borough code, if it's not then we will run into problems when we try to merge the \n",
    "#distance data back in later\n",
    "cdata.sort_values(by=[\"OrigCodeNew\", \"DestCodeNew\"], inplace = True)\n",
    "\n",
    "#reset the resulting index (important for later) when merging back together\n",
    "cdata.reset_index(inplace=True)\n",
    "cdata.drop(columns = [\"index\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish we need to add in our distance data that we generated earlier and create a new column of total flows that excludes the flows that occur within boroughs (we could keep the within-borough (intra-borough) flows in but they can cause problems so for now we will just exclude them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we create a new total column which excludes intra-borough flow totals (we will set them\n",
    "#to a very small number for reasons you will see later)\n",
    "cdata[\"TotalNoIntra\"] = cdata.apply(lambda x: 0 if x['OrigCode'] == x['DestCode'] else x['Total'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number to a very small number\n",
    "cdata[\"offset\"]= cdata.apply(lambda x: 0.0000000001 if x['OrigCode'] == x['DestCode'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the distance column into the dataframe\n",
    "cdata = cdata.merge(distances['Dist'], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at what the data now looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the flows that originate from the city of London\n",
    "cdata.head(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice but to make this demonstration easier, let's just select a small subset of these flows (we can come back to the whole dataset later on...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will just select the first 7 boroughs by code\n",
    "to_match = [\"00AA\", \"00AB\", \"00AC\", \"00AD\", \"00AE\", \"00AF\", \"00AG\"]\n",
    "\n",
    "#subset the data by the 7 sample boroughs\n",
    "#first the origins\n",
    "cdatasub = cdata[cdata[\"OrigCode\"].isin(to_match)]\n",
    "#then the destinations\n",
    "cdatasub = cdatasub[cdata[\"DestCode\"].isin(to_match)]\n",
    "\n",
    "#now chop out the intra-borough flows\n",
    "cdatasub = cdatasub[cdata[\"OrigCode\"] != cdata[\"DestCode\"]]\n",
    "\n",
    "#we now want to re-order so that OrigCodeNEw, DestCodeNew and TotalNoIntra are the first three columns\n",
    "beg = [\"OrigCodeNew\", \"DestCodeNew\", \"TotalNoIntra\"] \n",
    "cols = beg + [col for col in cdatasub.columns.tolist() if col not in beg]\n",
    "#re index the columns\n",
    "cdatasub = cdatasub.reindex(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatasub.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try plotting the flow lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the base map\n",
    "London.plot(figsize = (20,30), facecolor = \"None\", edgecolor = \"r\")\n",
    "\n",
    "#iterate over each pair\n",
    "for index, row in cdatasub.iterrows():\n",
    "    #get the x coordinates\n",
    "    x1 = [London[London[\"lad15cd\"] == row[\"OrigCodeNew\"]][\"Centroids\"].x, London[London[\"lad15cd\"] == row[\"DestCodeNew\"]][\"Centroids\"].x]\n",
    "    #get the y coordinates\n",
    "    y1 = [London[London[\"lad15cd\"] == row[\"OrigCodeNew\"]][\"Centroids\"].y, London[London[\"lad15cd\"] == row[\"DestCodeNew\"]][\"Centroids\"].y]\n",
    "    #plot the line between the x and y coordinates with width reflecting the number of flows\n",
    "    plt.plot(x1, y1, linewidth = row[\"TotalNoIntra\"]/max(cdatasub[\"TotalNoIntra\"])*10, color = \"Blue\")\n",
    "    \n",
    "#be careful when working with large amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you really want to be cool - on a leaflet map. See [here](https://python-visualization.github.io/folium/) for more details on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "#set the base map to locate on London\n",
    "my_map =folium.Map(location = [51.5, 0.1278], zoom_start = 10)\n",
    "\n",
    "#transform to WGS84\n",
    "London_WGS = London.to_crs(\"EPSG:4326\")\n",
    "London_WGS[\"x\"] = London_WGS.geometry.centroid.map(lambda p: p.y)\n",
    "London_WGS[\"y\"]= London_WGS.geometry.centroid.map(lambda p: p.x)\n",
    "\n",
    "#plot in leaflet\n",
    "#This is done using the folium library\n",
    "#iterate over each pair\n",
    "for index, row in cdatasub.iterrows():\n",
    "    #get the first set of coordinates\n",
    "    x1 = [London_WGS[London_WGS[\"lad15cd\"]==row[\"OrigCodeNew\"]][\"x\"].values[0],\n",
    "       London_WGS[London_WGS[\"lad15cd\"]==row[\"OrigCodeNew\"]][\"y\"].values[0]]\n",
    "    #get the second set of coordinates\n",
    "    x2 = [ London_WGS[London_WGS[\"lad15cd\"]==row[\"DestCodeNew\"]][\"x\"].values[0],\n",
    "           London_WGS[London_WGS[\"lad15cd\"]==row[\"DestCodeNew\"]][\"y\"].values[0]]\n",
    "    #create a list of the coordinates\n",
    "    coords = [x1, x2]\n",
    "    #add a polyline with size reflective of the number of people\n",
    "    folium.vector_layers.PolyLine(coords, weight = row[\"TotalNoIntra\"]/max(cdatasub[\"TotalNoIntra\"])*20, opacity = 0.5\n",
    "                                 ).add_to(my_map)\n",
    "                                \n",
    "#plot the map                                \n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#now we can create a pivot table to turn paired list into a matrix, and compute the margin as well\n",
    "cdatasubmat = pd.pivot_table(cdatasub, values =\"TotalNoIntra\", index=\"Orig\", columns = \"Dest\",\n",
    "                            aggfunc=np.sum, margins=True)\n",
    "cdatasubmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set everything up its"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellin' Time!\n",
    "\n",
    "In explaining how to run and calibrate spatial interaction models in Python, Mike Batty's notation will be used for spatial interaction models. An alternative for this would be using Taylor Oshan's notation in his excellent primer on interaction models in Python. The paper is well worth a read and can be found [here](http://openjournals.wu.ac.at/region/paper_175/175.html)\n",
    "\n",
    "Below is the classic multiplicate gravity model:\n",
    "\n",
    "\\begin{equation} \\label{eq:1} \\tag{1}\n",
    "T_{ij} = k \\frac{O_i^\\alpha  D_j^\\gamma}{ d_{ij}^\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "This gravity model can be written in the form more familiar from [Wilson's 1971 paper](http://journals.sagepub.com/doi/abs/10.1068/a030001) \n",
    "\n",
    "\\begin{equation} \\label{eq:2} \\tag{2}\n",
    "T_{ij} = k O_i^\\alpha  D_j^\\gamma  d_{ij}^{-\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "<b>This model just says that the flows between an origin and destination are proportional to the product of the mass of the origin and destination and inversley proportional to the distance between them.\n",
    "    \n",
    "As origin and destination masses increase, flows increase, but as distance increases, flows decrease, and vice versa.</b>\n",
    "\n",
    "- Where $T_{ij}$ is the transition or flow, $T$, between origin $i$ (always the rows in a matrix) and destination $j$ (always the columns in a matrix). If you are not overly familiar with matrix notation, the i and j are just generic indexes to allow us to refer to any cell in the matrix more generally.\n",
    "- $O$ is a vector (a 1 dimensional matrix - or, if you like, a single line of numbers) of origin attributes which relate to the emissiveness of all origins in the dataset, $i$ - in our sample dataset, we have a vector of origin populations (which I have called Oi1_origpop) and a vector of origin average salaries (which I have called Oi2_origsal) in 2001\n",
    "- $D$ is a vector of desination of attributes relating to the attractivenss of all destinations in the dataset, $j$ - in our sample dataset, we have a vector of destination populations (which I have called Dj1_destpop) and a vector of destination average salaries (which I have called Dj2_destsal) in 2001\n",
    "- $d$ is a matrix of costs relating to the flows between $i$ and $j$ - in our case the cost is the distance and it is called ‘dist’ in our dataset.\n",
    "- $k$, $\\alpha$, $\\gamma$ and $β$ are all the model parameters to be estimated\n",
    "\n",
    "$k$ is a constant of proportionality. Given that all flows estimated by the model will sum to any observed flow data used to calibrate the parameters:\n",
    "\n",
    "\\begin{equation} \\label{eq:3} \\tag{3}\n",
    "k = \\frac{T}{\\sum_i \\sum_j O_i^\\alpha  D_j^\\gamma  d_{ij}^{-\\beta}}\n",
    "\\end{equation}\n",
    "\n",
    "and $T$ is the sum of our matrix of observed flows or:\n",
    "\n",
    "\\begin{equation} \\label{eq:4} \\tag{4}\n",
    "T= \\sum_i \\sum_j T_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "In English, this is just the sum of all observed flows divided by the sum of all of the other elements in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Model Parameters\n",
    "\n",
    "Now, it’s perfectly possible to produce some flow estimates by plugging some arbitrary or expected estimated values into our parameters. The parameters relate to the scaling effect / importance of the variables they are associated with. Most simply, where the effects of origin and destination attributes on flows scale in a linear fashion (i.e. for a 1 unit increase in, say, population at origin, we might expect a 1 unit increase in flows of people from that origin, or for a halving in average salary at destination, we might expect a halving of commuters), $\\alpha$ = 1 and $\\gamma$ = 1. In Newton’s original gravity equation, $\\beta$ = 2 where the influence of distance on flows follows a power law - i.e. for a 1 unit increase in distance, we have a $1^{-2}$ (1) unit decrease in flows, for a 2 unit increase in distance, we have $2^{-2}$ (0.25 or 1/4) of the flows, for a 3 unit increase, $3^{-2}$ (0.111) etc.\n",
    "\n",
    "Let’s see if these parameters are a fair first guess (we’ll use the whole dataset in order to get a less messy picture)…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first plot the Total commuter flows denoted by T against distance denoted by d\n",
    "#and then fit a model line T ~ d^-beta with beta=2\n",
    "\n",
    "#set the base axis\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "#scatter plot\n",
    "ax.scatter(x = cdata[\"Dist\"], y=cdata[\"Total\"])\n",
    "#line plot\n",
    "line = np.arange(0.0002, 50_000, 0.1)\n",
    "ax.plot(line, line**-2, color =\"r\", label = \"$\\\\beta=2$\")\n",
    "#add a legend\n",
    "ax.legend(title = \"$T \\\\sim d^{-\\\\beta}$\", fontsize = 15, title_fontsize=15)\n",
    "#axis limits\n",
    "ax.set_xlim([-1000,50000])\n",
    "ax.set_ylim([-1000, 80000])\n",
    "#set the labels\n",
    "ax.set_xlabel(\"Distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Total Flows\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the origin and destination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us now look at the behaviour of the flows with respect to the population at the origin denoted by O\n",
    "#and then fit a model line T ~ O^alpha, for alpha=1\n",
    "\n",
    "#set the base axis\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "#scatter plot\n",
    "ax.scatter(x = cdata[\"Oi1_origpop\"], y=cdata[\"Total\"])\n",
    "#line plot\n",
    "line = np.arange(0.0002, 170_000, 0.1)\n",
    "ax.plot(line, line**1, color =\"r\", label = \"$\\\\alpha=1$\")\n",
    "#add a legend\n",
    "ax.legend( fontsize = 15, title = \"$T \\sim O^{\\\\alpha}$\", title_fontsize=15)\n",
    "#axis labels\n",
    "ax.set_xlabel(\"Origin population\", fontsize = 20)\n",
    "ax.set_ylabel(\"Total Flows\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us look at the behaviour of the flows with respect to the salaries at destination denoted by D\n",
    "# and then fit a model line  T ~ D^gamma. We will fit a line for gamma=1\n",
    "\n",
    "#set the base axis\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "#scatter plot\n",
    "ax.scatter(x = cdata[\"Dj2_destsal\"], y=cdata[\"Total\"])\n",
    "#line plot\n",
    "line = np.arange(15_000, 40000, 0.1)\n",
    "ax.plot(line, line**1, color =\"r\", label = \"$\\\\gamma=1$\")\n",
    "#add a legend\n",
    "ax.legend( fontsize = 15, title = \"$T \\sim D^{\\\\gamma}$\", title_fontsize=15)\n",
    "#axis labels\n",
    "ax.set_xlabel(\"Destination salary\", fontsize = 20)\n",
    "ax.set_ylabel(\"Total\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it looks like we are not far off (well, destination salary doesn't look too promising as a predictor, but we'll see how we get on ...), so let's see what flow estimates with these starting parameters look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up some variables to hold our parameter values in:\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "beta = 2\n",
    "k = 1\n",
    "T2 = sum(cdatasub[\"Total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create some flow estimates using equation 2 above... Begin by applying the parameters to the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Oi1_alpha = cdatasub[\"Oi1_origpop\"]**alpha\n",
    "Dj2_gamma = cdatasub[\"Dj2_destsal\"]**gamma\n",
    "dist_beta = cdatasub[\"Dist\"]**-beta\n",
    "T1 = Oi1_alpha*Dj2_gamma*dist_beta\n",
    "k = T2/sum(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, just as in Equation 2 above, just multiply everything together to get your flow estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the model and store of the new flow estimates in a new column\n",
    "cdatasub[\"unconstrainedEst1\"] = round(k*Oi1_alpha*Dj2_gamma*dist_beta, 0)\n",
    "#convert to integers\n",
    "cdatasub[\"unconstrainedEst1\"] = cdatasub[\"unconstrainedEst1\"].astype(int)\n",
    "#check that the sum of these estimates make sense\n",
    "sum(cdatasub[\"unconstrainedEst1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this into a matrix to look at the predictors flows we have produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatasubmat1 = cdatasub.pivot_table(values =\"unconstrainedEst1\", index=\"Orig\", columns = \"Dest\",\n",
    "                            aggfunc=np.sum, margins=True)\n",
    "cdatasubmat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the flow estimates compare to the original flows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatasubmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is my model?\n",
    "\n",
    "So looking at the two matrices above you can see that in some cases the flow estimates aren't too bad (Barking and Dagenham to Barnet for example, but in others they are pretty rubbish (Camden to the City of London?). Whilst it's OK to eyeball small flow matrics like this, when you have much larger matrices we need another solution ...\n",
    "\n",
    "### Testing the \"Goodness-of-fit\"\n",
    "\n",
    "Yes, that’s what it’s called - I know, it doesn’t sound correct, but goodness-of-fit is the correct term for checking how well your model estimates match up with your observed flows.\n",
    "\n",
    "So how do we do it?\n",
    "\n",
    "Well… there are a number of ways but perhaps the two most common are to look at the coefficient of determination ($r^2$) or the Square Root of Mean Squared Error (RMSE). You’ve probably come across $r^2$ before if you have fitted a linear regression model, but you may not have come across RMSE. There are other methods and they all do more or less the same thing, which is essentially to compare the modelled estimates with the real data. $r^2$ is popular as it is quite intuitive and can be compared across models. RMSE is less intuitive, but some argue that it is better for comparing changes to the same model. Here we’ll do both…\n",
    "\n",
    "### R-Squared\n",
    " \n",
    "$r^2$ is the square of the correlation coefficient, $r$\n",
    "\n",
    "For our sample data, we can calculate this very easily using a little function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def CalcRSqaured(observed, estimated):\n",
    "    \"\"\"Calculate the r^2 from a series of observed and estimated target values\n",
    "    inputs:\n",
    "    Observed: Series of actual observed values\n",
    "    estimated: Series of predicted values\"\"\"\n",
    "    \n",
    "    r, p = scipy.stats.pearsonr(observed, estimated)\n",
    "    R2 = r **2\n",
    "    \n",
    "    return R2\n",
    "\n",
    "CalcRSqaured(cdatasub[\"Total\"], cdatasub[\"unconstrainedEst1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function we get a value of around 0.50, or around 50%. This tells us that our model accounts for about 50% of the variation of flows in the system. Not bad, but not brilliant either.\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "We can use a similar simple function to calcualte the RMSE for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def CalcRMSE(observed, estimated):\n",
    "    \"\"\"Calculate Root Mean Square Error between a series of observed and estimated values\n",
    "    inputs:\n",
    "    Observed: Series of actual observed values\n",
    "    estimated: Series of predicted values\"\"\"\n",
    "    \n",
    "    res = (observed -estimated)**2\n",
    "    RMSE = round(sqrt(res.mean()), 3)\n",
    "    \n",
    "    return RMSE\n",
    "\n",
    "CalcRMSE(cdatasub[\"Total\"], cdatasub[\"unconstrainedEst1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure that is produced by the RMSE calcaultion is far less intuitive than the $r^2$ and this is mainly because it very much depends on things like the units the data are in and the volume of the data. It can't be used to compared different models run using different data sets. However, it is good for assessing whether changes to the model result in improvements. The closer to 0 the RMSE value, the better the model.\n",
    "\n",
    "So how can we start to improve our fit...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model: 1 -  calibrating parameters\n",
    "\n",
    "### (This bit might take a while but stick with it)\n",
    "\n",
    "Now, the model we have run above is probably the most simple spatial interaction model we could have run and the results aren’t terrible, but they’re not great either.\n",
    "\n",
    "One way that we can improve the fit of the model is by calibrating the parameters on the flow data that we have.\n",
    "\n",
    "The traditional way that this has been done computationally is by using the goodness-of-fit statistics. If you have the requisite programming skills, you can write a computer algorithm that iteratively adjusts each parameter, runs the model, checks the goodness-of-fit and then starts all over again until the goodness-of-fit statistic is maximised.\n",
    "\n",
    "This is partly why spatial interaction modelling was the preserve of specialists for so long as acquiring the requisite skills to write such computer programmes can be challenging!\n",
    "\n",
    "However, since the early days of spatial interaction modelling, a number of useful developments have occurred…\n",
    "\n",
    "The mathematically minded among you may have noticed that if you take the logarithms of both sides of Equation 2, you end up with the following equation:\n",
    "\n",
    "\\begin{equation} \\label{eq:5} \\tag{5}\n",
    "\\ln T_{ij} = K + \\alpha \\ln O_i + \\gamma \\ln D_j - \\beta \\ln d_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "where $K= \\ln k$. Those of you who have played around with regression models in the past will realise that this is exactly that - a regression model.\n",
    "\n",
    "And if you have played around with regression models you will be aware that there are various pieces of software available to run regressions (such as Python) and calibrate the parameters for us, so we don't have to be expert programmers to do this - yay!\n",
    "\n",
    "Now there are a couple of papers that are worth reading at this point. Perhaps the best is [Flowerdew and Aitkin (1982)](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9787.1982.tb00744.x/abstract), title “A METHOD OF FITTING THE GRAVITY MODEL BASED ON THE POISSON DISTRIBUTION”.\n",
    "\n",
    "One of the key points that Flowerdew and Aitkin make is that the model in equation 5 (known as a log-normal model) has various problems associated with it which means that the estimates produced might not be reliable. If you'd like to know more about these, read that paper (and also Wilson's 1971 paper), but at this point it is worth knowing that the way around many of these is to re-specify the model, not as a log-normal regression, but as a Poisson or negative binomial regression model.\n",
    "\n",
    "## Poisson regression\n",
    "\n",
    "The main theory (for non-experts like me anyway) behind the Poisson regression model is that the sorts of flows that spatial interaction models deal with (such as migration or commuting flows) relate to non-negative integer counts (you can’t have negative people moving between places and you can’t - normally, if they are alive - have fractions of people moving either).\n",
    "\n",
    "As such, the continuous (normal) probability distributions which underpin standard regression models don’t hold. However, the discrete probability distributions such as the Poisson distribution and the negative binomial distribution (of which the Poisson distribution is a special case - wikipedia it) do hold and so we can use these associations to model our flows.\n",
    "\n",
    "At this point, it’s probably worth you looking at what a Poisson disribution looks like compared to a normal distribution, if you are not familiar.\n",
    "\n",
    "Here’s a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "plt.hist(np.random.normal(loc = 75, scale= 3, size = 3000 ), bins = 50, edgecolor = \"grey\")\n",
    "plt.xlabel(\"Random normal distribution (0:3000, mean = 75, sd= 3)\", fontsize = 15)\n",
    "plt.ylabel(\"Count\", fontsize= 15)\n",
    "plt.title(\"Random normal distribution\", fontsize = 20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here’s a Poisson distribution with the same mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "plt.hist(np.random.poisson(lam = 75, size = 3000 ), bins = 50, edgecolor = \"grey\")\n",
    "plt.xlabel(\"Random Poisson distribution (0:3000, lambda = 75)\", fontsize = 15)\n",
    "plt.ylabel(\"Count\", fontsize= 15)\n",
    "plt.title(\"Random Poisson distribution\", fontsize = 20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks kind of similar doesn’t it! The thing with the Poisson distribution is, when the mean (λ - lambda) changes, so does the distribution. As the mean gets smaller (and this is often the case with flow data where small flows are very likely - have a look at the ‘Total’ column in your cdata dataframe, lots of small numbers aren’t there?) the distribution starts to look a lot more like a skewed or log-normal distrbution. The key thing is it’s not - it’s a Poisson distribution. Here’s a similar frequency distribution with a small mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "plt.hist(np.random.poisson(lam = 0.5, size = 3000))\n",
    "plt.xlabel(\"Random Poisson distribution (0:3000, lambda = 0.5)\", fontsize = 15)\n",
    "plt.ylabel(\"Count\", fontsize= 15)\n",
    "plt.title(\"Random Poisson distribution\", fontsize = 20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as we’re concerned, what this means is that if we are interested in all flows between all origins and destinations in our system, these flows will have a mean value of $\\lambda_{ij}$ and this will dictate the distribution. Here’s what the distrbution of our flows looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "plt.hist(cdata[\"Total\"], histtype=\"stepfilled\" , bins = 50)\n",
    "plt.xlabel(\"London travel to work flows histogram\", fontsize = 15)\n",
    "plt.ylabel(\"Count\", fontsize= 15)\n",
    "plt.title(\"London travel to work flows\", fontsize = 20)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mmmm, Poissony!\n",
    "\n",
    "So, what does all of this mean for our spatial interaction model?\n",
    "\n",
    "Well the main thing it means is that Equation 5, for most sorts of spatial interaction models where we are modelling flows of people or whole things, is not correct.\n",
    "\n",
    "By logging both sides of the equation in Equation 5, we are trying to get a situation where our Tij flows can be modelled by using the values of our other variables such as distance, by using a straight line a bit like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the dataframe to the flows we want\n",
    "cdata_flows = cdata[[\"Total\", \"Dist\"]]\n",
    "#remove all 0 values (logarithms can't deal with 0 values)\n",
    "cdata_flows = cdata_flows[(cdata_flows!=0).all(1)]\n",
    "\n",
    "#extract the x and y converting to log\n",
    "x = np.log(cdata_flows[\"Dist\"])\n",
    "y = np.log(cdata_flows[\"Total\"])\n",
    "\n",
    "#create the subplot\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "#plot the results along with the line of best fit\n",
    "sns.regplot(x=x, y=y, marker=\"+\", ax=ax)\n",
    "ax.set_xlabel(\"log(Dist)\", fontsize = 20)\n",
    "ax.set_ylabel(\"log(Total)\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this graph with the graph above (the first scatter plot we drew), it’s exactly the same data, but clearly by logging both the total and distance, we can get a bit closer to being able to fit a model estimate using a straight line.\n",
    "\n",
    "What the Poisson distribution means is that the y variable in our model corresponds to the expected value, as we will show in the equation (6) below, and the equation can be linearised and modelled in a similar way to the blue line above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Poisson Regression Spatial Interaction Model\n",
    "\n",
    "So, we can now re-specify Equation 5 as a Poisson Regression model. Basically, we are assuming that the flows follow a Poisson distribution, such that the expected value $\\lambda_{ij}$ verifies:\n",
    "\n",
    "\\begin{equation} \\label{eq:6} \\tag{6}\n",
    "\\lambda_{ij} = \\exp (K + \\alpha \\ln O_i + \\gamma \\ln D_j - \\beta \\ln d_{ij})\n",
    "\\end{equation}\n",
    "\n",
    "What this model says is that $\\lambda_{ij}$ (our independent variable - the estimate of $T_{ij}$) is <i>logarithmically linked </i> to (or modelled by) a linear combination of the logged independent variables in the model. \n",
    "\n",
    "Now we have Equation 6 at our disposal, we can use a Poisson regression model to produce estimates of $k$, $\\alpha$, $\\gamma$ and $\\beta$ - or put another way, we can use the regression model to calibrate our parameters.\n",
    "\n",
    "So, let's have a go at doing it!\n",
    "\n",
    "It is very straightforward to run a Poisson regression model in Python using the `glm` function in the Statsmodel.api library. In practical terms, running a GLM model is no different to running a standard regression model using `lm`. If you want to find more details about this you can visit the statsmodel api documentation [here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels.genmod.generalized_linear_model.GLM) and [here](https://www.statsmodels.org/stable/examples/notebooks/generated/glm_formula.html) or for more information about the Poisson regression application you can read other articles on the internet such as this [medium article](https://towardsdatascience.com/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958). If you delve far enough into the depths of what GLM does, you will find that the parameters are calibrated though an ‘iteratively re-weighted least squares’ algorithm. This algorithm does exactly the sort of job I described earlier, it fits lots of lines to the data, continually adjusting the parameters and then seeing if it can minimise the error between the observed and expected values using some goodness-of-fit measure.\n",
    "\n",
    "These sorts of algorithms have been around for years and are very well established so it makes sense to make use of them rather than trying to re-invent the wheel ourselves. So here we go…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#take the variables and produce logarithms of them\n",
    "x_variables = [\"Oi1_origpop\", \"Dj2_destsal\", \"Dist\"]\n",
    "log_x_vars = []\n",
    "for x in x_variables:\n",
    "    cdatasub[f\"log_{x}\"] = np.log(cdatasub[x])\n",
    "    log_x_vars.append(f\"log_{x}\")\n",
    "\n",
    "#create the formula\n",
    "formula = 'Total ~ log_Oi1_origpop + log_Dj2_destsal + log_Dist'\n",
    "\n",
    "#run the regression\n",
    "uncosim = smf.glm(formula = formula, data=cdatasub, family=sm.families.Poisson()).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a simple as that - runs in a matter of milliseconds. You should be able to see how the `glm` code corresponds to Equation 6.\n",
    "\n",
    "`Total` = $T_{ij}$ = $\\lambda_{ij}$\n",
    "\n",
    "`~` means ‘is modelled by’\n",
    "\n",
    "`log_Oi1_origpop` = $\\ln O_i$\n",
    "\n",
    "`log_Dj2_destsal` = $\\ln D_j$\n",
    "\n",
    "`log_dist` = $\\ln d_{ij}$\n",
    "\n",
    "`family=sm.families.Poisson()`means that we are using a Poisson regression model (the link is always log with a Poisson model) where the left-hand side of the model equation is logarithmically linked to the variables on the right-hand side.\n",
    "\n",
    "So what comes out of the other end?\n",
    "\n",
    "Well, we can use the `summary()` function to have a look at the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the summary of the constrained model\n",
    "print(uncosim.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the summary that the Poisson regression has calibrated all 4 parameters for us and these appear under the ‘estimate’ column:\n",
    "\n",
    "$K$ (intercept) = -15.8084. Recall $K=\\ln k$, hence $k$ can NEVER be negative!\n",
    "\n",
    "$\\alpha$ = 1.7558\n",
    "\n",
    "$\\gamma$ = 1.6472\n",
    "\n",
    "and $\\beta$ = 1.4079 recall in equation (6) there is a negative sign in front of $\\beta$. \n",
    "\n",
    "We can also see from the other outputs that all variables are highly significant (P>|z| < 0.01), with the z-scores revealing that distance has the most influence on the model (as we might have expected from the scatter plots we produced earlier which showed that distance had by far the strongers correlation with commuting flows).\n",
    "\n",
    "These parameters are not too far away from our initial guesses of $\\alpha$ = 1, $\\gamma$ = 1 and $\\beta$ = 2, but how do the estimates compare?\n",
    "\n",
    "One way to calculate the estimates is to plug all of the parameters back into Equation 6 like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first assign the parameter values from the model to the appropriate variables\n",
    "K = uncosim.params[0]\n",
    "alpha = uncosim.params[1]\n",
    "gamma = uncosim.params[2]\n",
    "beta = -uncosim.params[3]\n",
    "\n",
    "#now plug everything back into the Equation 6 model ... \n",
    "#be careful with the negative signing of the parameter beta\n",
    "cdatasub[\"unconstrainedEst2\"] = np.exp(K + alpha*cdatasub[\"log_Oi1_origpop\"] + gamma*cdatasub[\"log_Dj2_destsal\"] -\n",
    "                                      beta*cdatasub[\"log_Dist\"])\n",
    "\n",
    "#or we can just extract the results from the actual poisson regression and apply them to the data\n",
    "predictions = uncosim.get_prediction()\n",
    "predictions_summary_frame = predictions.summary_frame()\n",
    "cdatasub[\"fitted\"] = predictions_summary_frame[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the numbers so that we don't get a half of a person\n",
    "cdatasub[\"unconstrainedEst2\"] = round(cdatasub[\"unconstrainedEst2\"], 0)\n",
    "#convert to integers\n",
    "cdatasub[\"unconstrainedEst2\"] = cdatasub[\"unconstrainedEst2\"].astype(int)\n",
    "#check that the sum of these estimates make sense\n",
    "sum(cdatasub[\"unconstrainedEst2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn it into a little matrix and have a look at your handy work\n",
    "cdatasubmat2 = cdatasub.pivot_table(values =\"unconstrainedEst2\", index=\"Orig\", columns = \"Dest\",\n",
    "                            aggfunc=np.sum, margins=True)\n",
    "cdatasubmat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare it to the actual flows\n",
    "cdatasubmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that some of the estimates have improved such as Barking and Dagenham to Barnet is now even closer and is within 10% of the actual flow, while the Barking and Dagenham to Bexley predicted flow has converged but is still far out.\n",
    "\n",
    "Therefore the $1,000,000 question is - has calibrating the parameters improved the model...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcRSqaured(cdatasub[\"Total\"], cdatasub[\"unconstrainedEst2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcRMSE(cdatasub[\"Total\"], cdatasub[\"unconstrainedEst2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yes indeedy do!!\n",
    "\n",
    "The $r^2$ has improved from 0.50 to 0.67 and the RMSE has reduced from 2503.35 to 1892.62 so by calibrating our parameters using the Poisson Regression Model, we have markedly improved our model fit.\n",
    "\n",
    "But we can do even better. We have just been playing with the unconstrained model, by adding constraints into the model we can both improve our fit further AND start to do cool things like estimate transport trip distributions from known information about people leaving an area, or estaimte the amount of money a shop is going to make from the available money that people in the surrounding area have to spend, or guess the number of migrants travelling between specific countries where we only know how many people in total leave one country and arrive in another.\n",
    "\n",
    "We'll do all of this in part 2 of these sessions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension ideas for this session:\n",
    "\n",
    "1. Have a play around with inputting different parameter values and see what happens to the flow estimates - for example, what happens if you change the frictional effect of distance by increasing and decreasing the value of the $\\beta$ parameter? Try some values of 2, 3, 4. What do negative values do to the flows?\n",
    "\n",
    "\n",
    "2. What happens to the flow estimates if you adjust the other parameters in the model or remove $k$?\n",
    "\n",
    "\n",
    "3. Try running the model on the whole London system - although you may run into problems if you don’t remove the intra-borough flows. This is because for intra-flows, the distance value is 0 and you can’t take the log of 0. To fix this, you can either change all of the zero distance values to something very small (like 1 or 0.5) or, alternatively, remove all rows in the data where origin = destination with some code like this: `cdata2 = cdata[cdata[\"OrigCode\"] != cdata[\"DestCode\"]]`\n",
    "\n",
    "\n",
    "4. What might you expect to happen to the flows of people between, say, Camden and Brent if all of a sudden loads of really well paid jobs appeared in Brent and the average salary doubled? How might this impact the other Boroughs in the system?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export and save the results\n",
    "cdatasub.to_csv(\"Data/cdatasub1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbsim",
   "language": "python",
   "name": "urbsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
